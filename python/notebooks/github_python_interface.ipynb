{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GITHUB/PYTHON INTERFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - RUNNING THE PYTHON SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 existing files were updated since last pull\n",
      "generating new database...\n"
     ]
    }
   ],
   "source": [
    "%run pycovid/pycovid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datadotworld as dw\n",
    "import os\n",
    "import pandas as pd\n",
    "from pycovid import pycovidfunc as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_dataset = dw.load_dataset('psychopresley/covid19-tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'psychopresley_covid19-tracking',\n",
       " 'title': 'Covid19 Tracking',\n",
       " 'homepage': 'https://data.world/psychopresley/covid19-tracking',\n",
       " 'resources': [],\n",
       " 'profile': 'data-package'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = dw.api_client()\n",
    "api_client.upload_files('psychopresley/covid19-tracking',\n",
    "                        [r'C:\\Users\\user\\Documents\\GitHub\\COVID-19\\consolidated_data\\country_report.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client.update_dataset('psychopresley/covid19-tracking',\n",
    "                        files = {'country_report.json': {'url':'https://github.com/psychopresley/COVID-19/tree/master/consolidated_data/country_report.json',\n",
    "                                                         'description': 'automated update 07-25-2020'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid value for `title`, must not be `None`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f6ad80d6f507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m api_client.replace_dataset('psychopresley/covid19-tracking',\n\u001b[0;32m      2\u001b[0m                            files = {'country_report.json': {'url':'https://github.com/psychopresley/COVID-19/tree/master/consolidated_data/country_report.json',\n\u001b[1;32m----> 3\u001b[1;33m                                                          'description': 'automated update 07-25-2020'}})\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datadotworld\\client\\api.py\u001b[0m in \u001b[0;36mreplace_dataset\u001b[1;34m(self, dataset_key, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                 labels=labels),\n\u001b[1;32m--> 250\u001b[1;33m             kwargs)\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mowner_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_dataset_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datadotworld\\client\\api.py\u001b[0m in \u001b[0;36m__build_dataset_obj\u001b[1;34m(dataset_constructor, file_constructor, args)\u001b[0m\n\u001b[0;32m   1339\u001b[0m                      for name, file_info in args['files'].items()]\n\u001b[0;32m   1340\u001b[0m                  if 'files' in args else None)\n\u001b[1;32m-> 1341\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'title'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datadotworld\\client\\api.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    238\u001b[0m             lambda: _swagger.DatasetPutRequest(\n\u001b[0;32m    239\u001b[0m                 \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m                 \u001b[0mvisibility\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visibility'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m             ),\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_archive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datadotworld\\client\\_swagger\\models\\dataset_put_request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, description, summary, tags, license, visibility, files)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datadotworld\\client\\_swagger\\models\\dataset_put_request.py\u001b[0m in \u001b[0;36mtitle\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid value for `title`, must not be `None`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid value for `title`, length must be less than or equal to `60`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid value for `title`, must not be `None`"
     ]
    }
   ],
   "source": [
    "api_client.replace_dataset('psychopresley/covid19-tracking',\n",
    "                           files = {'country_report.json': {'url':'https://github.com/psychopresley/COVID-19/tree/master/consolidated_data/country_report.json',\n",
    "                                                         'description': 'automated update 07-25-2020'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pycovid import pycovidfunc as cv\n",
    "\n",
    "if 'config.csv' in os.listdir(os.getcwd()):\n",
    "    config = pd.read_csv('config.csv',index_col='var').fillna('-')\n",
    "else:\n",
    "    raise FileNotFoundError('No configuration file \"config.csv\" found.')\n",
    "    \n",
    "who_data_dir = config.loc['who_data_dir'].path\n",
    "who_file_list = os.listdir(who_data_dir)\n",
    "for file in who_file_list:\n",
    "    if not file.endswith('.csv'):\n",
    "        who_file_list.remove(file)\n",
    "\n",
    "df = cv.raw_data_formatter(who_file_list,who_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - PYTHON PYCOVID.PY SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pycovid import pycovidfunc as cv\n",
    "\n",
    "git_dir = r\"C:\\Program Files\\Git\\cmd\"\n",
    "git_bin = os.path.join(git_dir, \"git\")\n",
    "\n",
    "os.putenv(\"GIT_PYTHON_GIT_EXECUTABLE\", git_bin)\n",
    "os.environ.putenv(\"GIT_PYTHON_GIT_EXECUTABLE\", git_bin)\n",
    "\n",
    "# Making sure that it is first in PATH\n",
    "sys.path = [git_dir] + sys.path\n",
    "os.environ[\"PATH\"] = os.pathsep.join([git_dir]) + os.pathsep + os.environ[\"PATH\"]\n",
    "\n",
    "# Only import git now, because that's when the path is checked!\n",
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing files were updated\n",
      "0 new files found. No further action necessary\n"
     ]
    }
   ],
   "source": [
    "# Read the config file to check for data file information:\n",
    "\n",
    "if 'config.csv' in os.listdir(os.getcwd()):\n",
    "    config = pd.read_csv('config.csv',index_col='var').fillna('-')\n",
    "else:\n",
    "    raise FileNotFoundError('No configuration file \"config.csv\" found.')\n",
    "    \n",
    "who_data_dir = config.loc['who_data_dir'].path\n",
    "repo = git.Repo(config.loc['git_repo'].path)\n",
    "\n",
    "# upstream repository\n",
    "upstream_repo = repo.remotes.upstream\n",
    "\n",
    "# Pull upstream base repo and check for modified files:\n",
    "lm_frame = cv.get_date_modified(who_data_dir)\n",
    "\n",
    "g = git.Git(upstream_repo)\n",
    "g.pull('upstream','master')\n",
    "\n",
    "lm_after_git_pull = cv.get_date_modified(who_data_dir)\n",
    "count_modified = 0\n",
    "\n",
    "for idx in lm_frame.index:\n",
    "    new_last_modified = lm_after_git_pull.loc[idx].last_modified\n",
    "    if lm_frame.loc[idx].last_modified != new_last_modified:\n",
    "        count_modified += 1\n",
    "\n",
    "who_file_list = os.listdir(who_data_dir)\n",
    "for file in who_file_list:\n",
    "    if not file.endswith('.csv'):\n",
    "        who_file_list.remove(file)\n",
    "\n",
    "# Compare the latest WHO file to the raw data update information\n",
    "# and calculates the number of files to update:\n",
    "\n",
    "flag = True # flag to indicate update\n",
    "if count_modified != 0:\n",
    "    print('{} existing files were updated since last pull'.format(count_modified))\n",
    "    print('generating new database...')\n",
    "    try:\n",
    "        df = cv.raw_data_formatter(who_file_list,who_data_dir)\n",
    "        new_date = datetime.strftime(datetime.now().date(),format='%m-%d-%Y')\n",
    "\n",
    "        raw_data_path = config.loc['raw_data'].path\n",
    "        config.loc['raw_data'].last_update = new_date\n",
    "\n",
    "        df.to_csv(raw_data_path, index=False)\n",
    "        config.to_csv('config.csv')\n",
    "\n",
    "        print('new database generated succesfully!')\n",
    "    except:\n",
    "        print('process aborted. No new database generated.')\n",
    "else:\n",
    "    last_update = pd.to_datetime(config.loc['raw_data'].last_update)\n",
    "    latest_who_file_date = pd.to_datetime(who_file_list[-1].split(sep='.')[0])\n",
    "\n",
    "    files_to_update = (latest_who_file_date - last_update).days\n",
    "\n",
    "    # Generating the list of new files to update the database\n",
    "    if files_to_update != 0:\n",
    "        list_of_new_files = []\n",
    "        for i in list(range(1,files_to_update + 1)):\n",
    "            new_date = datetime.strftime((last_update\n",
    "                                          + timedelta(days=i)).date(),\n",
    "                                          format='%m-%d-%Y')\n",
    "            list_of_new_files.append(new_date + '.csv')\n",
    "    \n",
    "        # Generating a dataframe with new information:\n",
    "        df = cv.raw_data_formatter(list_of_new_files,who_data_dir)\n",
    "\n",
    "        # Appending the new data to existing raw data file and updating\n",
    "        # the raw data information in the config file:\n",
    "\n",
    "        raw_data_path = config.loc['raw_data'].path\n",
    "        config.loc['raw_data'].last_update = new_date\n",
    "\n",
    "        df.to_csv(raw_data_path, mode='a', index=False, header=None)\n",
    "        config.to_csv('config.csv')\n",
    "        print('No existing files were updated')\n",
    "        print('%d new file(s) found. All files appended into the raw data file' \n",
    "              % (files_to_update))\n",
    "    else:\n",
    "        flag = False\n",
    "        print('No existing files were updated')\n",
    "        print('0 new files found. No further action necessary')\n",
    "\n",
    "# Create the world data report from the raw data if any update in the raw data file:\n",
    "if flag:\n",
    "    print('Creating world data file...')\n",
    "    try:\n",
    "        df = pd.read_csv(config.loc['raw_data'].path)\n",
    "        country_report = cv.world_data_formatter(df)\n",
    "        country_report.to_json(config.loc['formatted_data'].path,orient='records')\n",
    "        print('World data report created succesfully!')\n",
    "\n",
    "        new_date = datetime.strftime(datetime.now().date(),format='%m-%d-%Y')\n",
    "        config.loc['formatted_data'].last_update = new_date\n",
    "        config.to_csv('config.csv')\n",
    "        \n",
    "        # Commit changes to github:\n",
    "        cv.commit_to_repo(repo)\n",
    "        cv.repo_info(repo)\n",
    "    except:\n",
    "        print('World data report creation aborted. Please verify the raw data file.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python/notebooks/github_python_interface.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(repo.git.diff(None, name_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headcommit = repo.head.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
